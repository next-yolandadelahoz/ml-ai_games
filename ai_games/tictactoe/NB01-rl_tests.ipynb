{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Tests on Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "In this notebook I am going to test wich strategy is better to train a RL agent in a very-limited environment such as the tic-tac-toe game.\n",
    "\n",
    "\n",
    "## RL algorithm\n",
    "The implemented algorithm is the [Q-Learning algorithm](https://en.wikipedia.org/wiki/Q-learning#Algorithm), that is able to find the optimal policy $\\pi^*$.\n",
    "\n",
    "$Q$ is a fuction that given a state and an action, returns a number that means the *quality*, so the best move for a state is the one that maximizes the expected value of the total reward over all successive steps. In this case, winning the game.\n",
    "\n",
    "The easiest way to implement the function $Q$ is as a matrix ($State \\times Action$). And if the transition probability matrix is not known, then we have to sample from the environment by making the agent to play. This way is posible to calculate $Q$ iteratively.\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{new} (s_{t},a_{t}) \\leftarrow (1-\\alpha) \\cdot \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} \\bigg) }^{\\text{learned value}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "* $r_{t}$ is the reward observed for the current state * $s_t$\n",
    "* $\\alpha \\in [0,1]$ is the learning rate, which represents the importance between previous experiences and the current one.\n",
    "* $\\gamma \\in [0,1]$ is the discount factor, which represents the difference in importance between future rewards and present rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from players.minimax import Minimax\n",
    "from players.qlearner import QLearner\n",
    "from players.random import Random\n",
    "from utils import train_player_seconds, test_players, train_player_games\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teachers & Learners\n",
    "\n",
    "### Random teacher\n",
    "The random teacher chooses a random move each turn. It does not aim to win or lose but it plays very fast.\n",
    "\n",
    "### Minimax teacher\n",
    "The minimax teacher aims to win. This agent is optimal in the sense that it can only win or draw a game. It is slow.\n",
    "\n",
    "### RL teacher\n",
    "What if I make a RL agent to play against itself? Will it learn? On each game it learns to play as player 1 and player 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "teachers = {\n",
    "    'random': Random(2),\n",
    "    'minimax': Minimax(2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "learners_seconds = {\n",
    "    'random': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': teachers['random'],\n",
    "        'results': {},\n",
    "        'train_func': lambda board,learner,teacher, game, checkpoint: learner._train_1_game(0.1, lambda x: 1-(x/10), board, teacher, game, checkpoint)\n",
    "    },\n",
    "    'minimax': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': teachers['minimax'],\n",
    "        'results': {},\n",
    "        'train_func': lambda board,learner,teacher, game, checkpoint: learner._train_1_game(0.1, lambda x: 1-(x/10), board, teacher, game, checkpoint)\n",
    "    },\n",
    "    'rl': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': None,  # Itself, referenced in future\n",
    "        'results': {},\n",
    "        'train_func': lambda board, learner, teacher, game, checkpoint: learner._autotrain_1_game(0.1, lambda x: 1-(x/10), board, game, checkpoint)\n",
    "    },\n",
    "}\n",
    "\n",
    "learners_seconds['rl']['teacher'] = learners_seconds['rl']['agent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments (limited playing time)\n",
    "I am going to train three RL agents with the Q-learning algorithm. To make the experiment fair, I will limit the resources the agents have by setting a fixed trainning time. This way, if an agent has a better but costly in (CPU operations) teacher, will play less games than other with a worse but faster teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_SECONDS = 60  # Time for training (sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training rl (random) for 60 seconds\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f83ff07e1fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_func'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_SECONDS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t {} games played\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'games'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Escritorio/workspace/TEC_LAB/ReinforcementLearning/ml-ai_games/ai_games/tictactoe/utils.py\u001b[0m in \u001b[0;36mtrain_player_seconds\u001b[0;34m(learner, teacher, train_func, seconds, checkpoint)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mgames_played\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# As player 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5f31338e1fcc>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(board, learner, teacher, game, checkpoint)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'teacher'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mteachers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'results'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m'train_func'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_1_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     },\n\u001b[1;32m      8\u001b[0m     'minimax': {\n",
      "\u001b[0;32m~/Escritorio/workspace/TEC_LAB/ReinforcementLearning/ml-ai_games/ai_games/tictactoe/players/qlearner.py\u001b[0m in \u001b[0;36m_train_1_game\u001b[0;34m(self, lr, f_df, board, opponent, games, checkpoint)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mQ_diff_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Don't know reward, play more...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_diff_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_1_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mgames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mdepth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# I know how good the action is (reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Escritorio/workspace/TEC_LAB/ReinforcementLearning/ml-ai_games/ai_games/tictactoe/players/qlearner.py\u001b[0m in \u001b[0;36m_train_1_game\u001b[0;34m(self, lr, f_df, board, opponent, games, checkpoint)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mQ_diff_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Don't know reward, play more...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_diff_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_1_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mgames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mdepth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# I know how good the action is (reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Escritorio/workspace/TEC_LAB/ReinforcementLearning/ml-ai_games/ai_games/tictactoe/players/qlearner.py\u001b[0m in \u001b[0;36m_train_1_game\u001b[0;34m(self, lr, f_df, board, opponent, games, checkpoint)\u001b[0m\n\u001b[1;32m    142\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_diff_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Escritorio/workspace/TEC_LAB/ReinforcementLearning/ml-ai_games/ai_games/tictactoe/players/qlearner.py\u001b[0m in \u001b[0;36m_train_1_game\u001b[0;34m(self, lr, f_df, board, opponent, games, checkpoint)\u001b[0m\n\u001b[1;32m    142\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_diff_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/pycharm-professional/171/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydev_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSTATE_SUSPEND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m                     \u001b[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/pycharm-professional/171/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# IFDEF CYTHON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/pycharm-professional/171/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/snap/pycharm-professional/171/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "ckpt_seconds = 10\n",
    "\n",
    "for rl_name, rl in learners_seconds.items():\n",
    "    rl['seconds'] = TRAIN_SECONDS\n",
    "    print(\"Training rl ({}) for {} seconds\".format(rl_name, TRAIN_SECONDS))\n",
    "    rl['games'] = train_player_seconds(\n",
    "        learner=rl['agent'],\n",
    "        teacher=rl['teacher'],\n",
    "        train_func=rl['train_func'],\n",
    "        seconds=TRAIN_SECONDS,\n",
    "        checkpoint=ckpt_seconds\n",
    "    )\n",
    "    print(\"\\t {} games played\\n\".format(rl['games']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics\n",
    "To measure the actual performance of each agent, I will make them play against several opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TEST_N_GAMES = 100  # Games to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal (minimax)\n",
    "If the agent is good enough, when faced against this oponent, no one will win and all the games will be draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_learners_vs_minimax(learners, n_games):\n",
    "    for rl_name, rl in learners.items():\n",
    "        print(\"Testing rl ({}) VS minimax ({} games)\".format(rl_name, n_games))\n",
    "        print(\"\\tAs player 1\")\n",
    "        results = test_players(rl['agent'], teachers['minimax'], n_games)\n",
    "        rl['results']['vs_minimax (p1)'] = {\n",
    "            'wins': results[1],\n",
    "            'loses':  results[2],\n",
    "            'draws':  results[-1]\n",
    "        }\n",
    "        print(rl['results']['vs_minimax (p1)'])\n",
    "        print(\"\\tAs player 2\")\n",
    "        results = test_players(teachers['minimax'], rl['agent'], n_games)\n",
    "        rl['results']['vs_minimax (p2)'] = {\n",
    "            'wins': results[2],\n",
    "            'loses':  results[1],\n",
    "            'draws':  results[-1]\n",
    "        }\n",
    "        print(rl['results']['vs_minimax (p2)'])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print_learners_vs_minimax(learners_seconds, TEST_N_GAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Against other RL agent\n",
    "However, what the agent has learned could be a non-optimal policy and this way I could choose which one is the best. There can only be one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def print_learners_matches(learners, n_games):\n",
    "    for (rl1_name, rl1), (rl2_name,rl2) in itertools.combinations_with_replacement(learners.items(),2):\n",
    "        print(\"Testing rl ({}) VS rl({}) ({} games)\".format(rl1_name, rl2_name, n_games))\n",
    "        print(\"\\tAs player 1\")\n",
    "        results = test_players(rl1['agent'], rl2['agent'], n_games)\n",
    "        rl['results']['vs_{} (p1)'.format(rl2_name)] = {\n",
    "            'wins': results[1],\n",
    "            'loses':  results[2],\n",
    "            'draws':  results[-1]\n",
    "        }\n",
    "        print(rl['results']['vs_{} (p1)'.format(rl2_name)])\n",
    "        print(\"\\tAs player 2\")\n",
    "        results = test_players(rl2['agent'], rl1['agent'], n_games)\n",
    "        rl['results']['vs_{} (p2)'.format(rl2_name)] = {\n",
    "            'wins': results[2],\n",
    "            'loses':  results[1],\n",
    "            'draws':  results[-1]\n",
    "        }\n",
    "        print(rl['results']['vs_{} (p2)'.format(rl2_name)])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print_learners_matches(learners_seconds, TEST_N_GAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of scope\n",
    "This notebook does not include how to find the best hyperparameters, it's just a glimpse on how to explore the search space and proving that in order to learn how to win, the agent must win during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Experiments (limited number of games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_GAMES = 2000  # Number of games for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "learners_games = {\n",
    "    'random': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': teachers['random'],\n",
    "        'results': {},\n",
    "        'train_func': lambda board,learner,teacher: learner._train_1_game(0.1, lambda x: 1-(x/10), board, teacher)\n",
    "    },\n",
    "    'minimax': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': teachers['minimax'],\n",
    "        'results': {},\n",
    "        'train_func': lambda board,learner,teacher: learner._train_1_game(0.1, lambda x: 1-(x/10), board, teacher)\n",
    "    },\n",
    "    'rl': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': None,  # Itself, referenced in future\n",
    "        'results': {},\n",
    "        'train_func': lambda board, learner, teacher: learner._autotrain_1_game(0.1, lambda x: 1-(x/10), board)\n",
    "    },\n",
    "}\n",
    "\n",
    "learners_games['rl']['teacher'] = learners_seconds['rl']['agent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ckpt_games = 10\n",
    "\n",
    "for rl_name, rl in learners_games.items():\n",
    "    rl['games'] = TRAIN_GAMES\n",
    "    print(\"Training rl ({}) in {} games\".format(rl_name, TRAIN_GAMES))\n",
    "    rl['games'] = train_player_games(\n",
    "        learner=rl['agent'],\n",
    "        teacher=rl['teacher'],\n",
    "        train_func=rl['train_func'],\n",
    "        games=TRAIN_GAMES,\n",
    "        checkpoint = ckpt_games\n",
    "    )\n",
    "    print(\"\\t {} seconds played\\n\".format(rl['games']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TEST_N_GAMES = 100  # Games to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Against Minimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print_learners_vs_minimax(learners_games, TEST_N_GAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Against other RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print_learners_matches(learners_games, TEST_N_GAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Test the bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "learners_best = {\n",
    "    'seconds_rl': learners_seconds['rl'],\n",
    "    'games_minimax': learners_games['minimax']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print_learners_matches(learners_best, TEST_N_GAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "These are the knowledge extracted from the experiment.\n",
    "\n",
    "\n",
    "### On learning efficiency\n",
    "The TicTacToe game is an easy environment for an agent as the number of different states is finite and small in contrast with a real-world problem.\n",
    "\n",
    "In order to overcome the fact that the actual computing power allows brute-force searching strategies thus making the game a solved one, some kind of restrictions have to be set. These constraints delimit a tradeoff between the strength (quality of the actions) of the opponent the agent will have to face and the number of games it is able to play (pairs state-action in the Q function) and enables the comparison of different exploration/learning strategies.\n",
    "\n",
    "#### Time limited\n",
    "When the time to learn is fixed, having a quick response from the opponent allows to play more games. More games equals more knowledge about the game. This way, a random teacher overpowers the ability to train a RL agent of a minimax (optimal move) teacher. Although this randomly explored knowledge may not be of the best quality, it could be enough to explore all the states and therefore, know all the game.\n",
    "\n",
    "\n",
    "#### Games limited\n",
    "When the limitation is on the number of games to play, having an optimal player as a teacher is better than exploring randomly the game.\n",
    "\n",
    "\n",
    "#### Minimax teacher\n",
    "The problem with training against the minimax teacher is that the agent won't be able to learn how to win (strategy) as the minimax algorithm never loses, it wins or forces draws. So, the strategy the agent learns will be conservative as some states have not be explored. It can be seen in the game-limited experiment, the rl-minimax agent performs beter than the rl-rl agent against a minimax player but, between them the rl-rl wins.\n",
    "\n",
    "\n",
    "#### auto-double learning\n",
    "When the agent trains against itself, it is able to learn as player 1 and player 2 in the same game. This speeds-up the process and allows the agent to selectively (not randomly or optimally) explore the game graph. The performance is good in both cases, time and number-of-games-games constrained, as it is as good or even better than other agents. It generalizes very well. It is like having a teacher with the speed similar to a random one and the quality that matches the optimal one as more games are played.\n",
    "\n",
    "*Note: Double learning can be done because an agent playing as player 1 has a completely different set of states as the same playing as player 2. They are 2 completely different agents.*\n",
    "\n",
    "\n",
    "### Future experiments\n",
    "Some ideas for the future.\n",
    "\n",
    "#### Starting point\n",
    "It will be very interesting to explore how to embed past experiences and knowledge from the environment into the agent (i.e. human experience) to have a starting point to start the training and avoid a cold start."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-aafaf002",
   "language": "python",
   "display_name": "PyCharm (ml-ai_games)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}