{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "\n",
    "### Nomenclature\n",
    "1. $S$: set of states.\n",
    "1. $A$: set of actions.\n",
    "1. $R$: set of rewards (per states).\n",
    "1. $\\pi$: optimal policy.\n",
    "1. $T^\\pi$: policy transition matrix of size $n√ón$ containing state transition probabilities for always choosing $\\pi(s)$ in each respective state.\n",
    " \n",
    "\n",
    "\n",
    "### Assumptions\n",
    "1. Finite, small state spaces.\n",
    "1. We know the expert's policy $\\pi$. \n",
    "1. $\\pi$ is optimal and stationary deterministic.\n",
    "\n",
    "\n",
    "### Linear problem\n",
    "Get the rewards $R$ for the states under an optimal policy $\\pi$.\n",
    "\n",
    "$$\n",
    "\\forall \\mathbf{T}^i \\in \\mathbf{T}^{\\neg\\pi}: (\\mathbf{T}^\\pi-\\mathbf{T}^i)(\\mathbf{I} - \\gamma\\mathbf{T}^\\pi)^{-1} \\mathbf{R} \\succeq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the optimum policy Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3f61dc75d51f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinprog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "from board import Board\n",
    "from players.minimax import Minimax\n",
    "from players.qlearner import QLearner\n",
    "from utils import test_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "agent =  QLearner(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "TRAIN_SECONDS = 10*60\n",
    "\n",
    "games_played = 0\n",
    "time_start = time.time()\n",
    "time_stop = time.time() + TRAIN_SECONDS\n",
    "while time.time() < time_stop:\n",
    "    agent._autotrain_1_game(\n",
    "        lr=0.1, \n",
    "        f_df=lambda x: 1-(x/10),\n",
    "        board=Board()\n",
    "    )\n",
    "    games_played += 1\n",
    "\n",
    "print(\"\\t {} games played\\n\".format(games_played))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the agent, should tie (-1) against an optimum player (i.e. minimax)\n",
    "minimax = Minimax(1)\n",
    "N_GAMES = 100\n",
    "\n",
    "print(\"Testing agent VS minimax ({} games)\".format(N_GAMES))\n",
    "print(\"\\tAs player 1\")\n",
    "print(test_players(agent, minimax, N_GAMES))\n",
    "print(\"\\tAs player 2\")\n",
    "print(test_players(minimax, agent, N_GAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear problem\n",
    "\n",
    "Original formula:\n",
    "$$\n",
    "\\forall \\mathbf{T}^i \\in \\mathbf{T}^{\\neg\\pi}: (\\mathbf{T}^\\pi-\\mathbf{T}^i)(\\mathbf{I} - \\gamma\\mathbf{T}^\\pi)^{-1} \\mathbf{R} \\succeq 0\n",
    "$$\n",
    "\n",
    "Formula adapted to solver:\n",
    "$$\n",
    "\\forall \\mathbf{T}^i \\in \\mathbf{T}^{\\neg\\pi}: (\\mathbf{T}^i-\\mathbf{T}^\\pi)(\\mathbf{I} - \\gamma\\mathbf{T}^\\pi)^{-1} \\mathbf{R} \\preceq 0\n",
    "$$\n",
    "\n",
    "*Notes:*\n",
    "* T_factor1 $= (\\mathbf{T}^i-\\mathbf{T}^\\pi)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Q = agent.Q\n",
    "\n",
    "T_pi = {  # Transition probabilities under optimal policy (always 1)\n",
    "    #'val': [], # Transition probability (1)\n",
    "    'row': [], # Origin state\n",
    "    'col': []  # Destination state\n",
    "}\n",
    "\n",
    "T_factor1 = {  # Precalculated inecuation (T^i - T^pi)\n",
    "    # T^i is 1, T^pi is -1, ultra-sparse matrix\n",
    "    'val': [],\n",
    "    'row': [],\n",
    "    'col': []\n",
    "}\n",
    "\n",
    "# Index state->position in matrix\n",
    "index_states = {state:state_i for state_i,state in enumerate(Q.keys())}\n",
    "\n",
    "def get_position(state):  # TODO, will be a method in a class\n",
    "    '''Get the position of state in the index.\n",
    "    If not in the index, append at the end.\n",
    "    '''\n",
    "    retval = index_states.get(state)  # Destination state\n",
    "    if not retval:  # If state not in Q (final state), add to index\n",
    "        retval = len(index_states)\n",
    "        index_states[state] = retval\n",
    "    return retval\n",
    "    \n",
    "\n",
    "try:\n",
    "    ineq_i = 0  # inequation index\n",
    "    for state,state_i in list(index_states.items()):  # Make it a list, dict will change on the run\n",
    "        # Populate T_pi, take the best action and add it.\n",
    "        actions = list(Q[state].items())   # [(action, score)]\n",
    "        best_action = max(actions, key=lambda x: x[1])\n",
    "        row,col = best_action[0] # Move in the board\n",
    "        board = Board(board=state)\n",
    "        board.move(row,col,1)  # Defensive programming, better execute action than creating m\n",
    "        #T_pi['val'].append(1)  # Transition is deterministic 1\n",
    "        T_pi['row'].append(state_i) # Orgin state\n",
    "        state_i_dest_opt = get_position(board.get_state())\n",
    "        T_pi['col'].append(state_i_dest_opt)\n",
    "\n",
    "        actions.remove(best_action)\n",
    "        # Calculate T_i lines for that state, non-optimal moves\n",
    "        for (row,col),score in actions:\n",
    "            board = Board(board=state)\n",
    "            board.move(row,col,1)\n",
    "            # Non-optimal move\n",
    "            T_factor1['row'].append(ineq_i)\n",
    "            T_factor1['col'].append(get_position(board.get_state()))\n",
    "            T_factor1['val'].append(1) # 1-0\n",
    "            # Optimal move\n",
    "            T_factor1['row'].append(ineq_i)\n",
    "            T_factor1['col'].append(state_i_dest_opt)\n",
    "            T_factor1['val'].append(-1) # 0-1    \n",
    "            ineq_i += 1 # Next inequation \n",
    "except Exception as e:\n",
    "    print(state)\n",
    "    print(actions)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "n_states = len(index_states)\n",
    "\n",
    "M_factor1 = sparse.csr_matrix(\n",
    "    (T_factor1['val'], (T_factor1['row'], T_factor1['col'])),\n",
    "    shape=(max(T_factor1['row'])+1, n_states)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.1\n",
    "\n",
    "M_factor2 = linalg.inv(\n",
    "    sparse.identity(n_states) - \n",
    "    (DISCOUNT_FACTOR * sparse.csr_matrix( \n",
    "        (np.ones(len(T_pi['row'])), (T_pi['row'], T_pi['col'])),\n",
    "        shape=(n_states, n_states)\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "M_ineq = M_factor1.dot(M_factor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of states: \"+str(n_states))\n",
    "print(\"Shape inequations matrix: \")\n",
    "print(M_ineq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = linprog(\n",
    "    c = np.ones(n_states),  # Function coefficients\n",
    "    A_ub = M_ineq.todense(),          # Constraint coefficients, this<=_\n",
    "    b_ub = np.zeros(M_ineq.shape[0])  # Constraint values _<=this \n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A degenerate solution\n",
    "\n",
    "The problem is I get a degenerate solution.\n",
    "\n",
    "Rewards equal to zero is a solution to the inequations. To filter this solution I will try to change the function to optimize.\n",
    "\n",
    "$$\n",
    "\\text{maximize} : \\sum_{s\\in\\mathcal{S}} (Q^\\pi(s, \\pi(s))-\\text{max}_{a\\in\\mathcal{A}\\setminus \\pi(s)}Q^\\pi(s, a))\n",
    "$$\n",
    "\n",
    "Maximize the difference between the first and second best solutions.\n",
    "\n",
    "Expressed as a minimization problem:\n",
    "$$\n",
    "\\text{minimize} : \\sum_{s\\in\\mathcal{S}} (Q^\\pi(s, \\pi(s))+\\text{max}_{a\\in\\mathcal{A}\\setminus \\pi(s)}Q^\\pi(s, a))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "coeficients = np.zeros(n_states)  # How should I model final states? No transitions.\n",
    "\n",
    "for state,state_i in list(index_states.items()):  # Make it a list, dict will change on the run\n",
    "    # Populate T_pi, take the best action and add it.\n",
    "    actions = list(Q[state].items())   # [(action, score)]\n",
    "    if actions:\n",
    "        best_action = _,score1 = max(actions, key=lambda x: x[1])\n",
    "        actions.remove(best_action)\n",
    "        if actions:\n",
    "            _,score2 = max(actions, key=lambda x: x[1])\n",
    "        else:\n",
    "            score2 = 0\n",
    "        coeficients[state_i] = score1 + score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper bound for Rewards (coeficients)\n",
    "$$\n",
    "\\forall s \\in \\mathcal{S}: |\\hat{\\mathcal{R}}(s)| \\leq R_{\\text{max}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M_ineq_coeffs = np.identity(n_states)\n",
    "R_max = 1 # No reward higher than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Z = M_ineq.todense()\n",
    "Z = np.vstack([Z,M_ineq_coeffs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of states: \"+str(n_states))\n",
    "print(\"Shape inequations matrix: \")\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "result = linprog(\n",
    "    c = coeficients,  # Function coefficients, minimize this function\n",
    "    A_ub = Z,          # Constraint coefficients, this<=_\n",
    "    b_ub = np.array(\n",
    "        [0]*M_ineq.shape[0] +\n",
    "        [R_max]*n_states\n",
    "    ),  # Constraint values _<=this \n",
    "    options={\n",
    "        'maxiter':5000  # With 1K iters do not converge\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "list(result.x!=0).count(True)  # States better than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rev = {i:board for board,i in index_states.items()} # Reverse index state(int) -> board(tuple)\n",
    "R = {rev[i]:v for i,v in enumerate(result.x) if v!=0}  # State(board) -> Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: if there are same number of 1s and 2s, the last move was made by player 2, it is a desirable state for them. \n",
    "Otherwise, more 1s than 2s, player 1 was the last to move, they try to reach this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for b in list(R.keys())[:10]:\n",
    "    print(Board(board=b))\n",
    "    print(\"Good state for player {}\\n\\n\".format(1 if b.count(1)>b.count(2) else 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symmetric states should have the same reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(R.get((2, 0, 0, 0, 0, 0, 1, 0, 1),0))\n",
    "print(R.get((2, 0, 1, 0, 0, 0, 0, 0, 1),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize and save the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"rewards.pickle\", \"+wb\") as fd:\n",
    "    pickle.dump( R, fd )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Productivize IRL Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from players.irl_finite import IRLFinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "agent_irl = IRLFinite(1,\"rewards.pickle\")\n",
    "\n",
    "\n",
    "# Test the agent, should tie (-1) against an optimum player (i.e. minimax)\n",
    "\n",
    "\n",
    "print(\"Testing agent IRL VS minimax ({} games)\".format(N_GAMES))\n",
    "print(\"\\tAs player 1\")\n",
    "print(test_players(agent_irl, minimax, N_GAMES))\n",
    "print(\"\\tAs player 2\")\n",
    "print(test_players(minimax, agent_irl, N_GAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "agent_irl = IRLFinite(1,\"rewards.pickle\")\n",
    "\n",
    "\n",
    "# Test the agent, should tie (-1) against an optimum player (i.e. minimax)\n",
    "\n",
    "\n",
    "print(\"Testing agent IRL VS agent ({} games)\".format(N_GAMES))\n",
    "print(\"\\tAs player 1\")\n",
    "print(test_players(agent_irl, agent, N_GAMES))\n",
    "print(\"\\tAs player 2\")\n",
    "print(test_players(agent, agent_irl, N_GAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* [Tutorial](https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html)\n",
    "* [Optimization library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html)\n",
    "* [Simplex doc](https://docs.scipy.org/doc/scipy/reference/optimize.linprog-simplex.html)\n",
    "* [Linear programming implementation](https://github.com/yrlu/irl-imitation/blob/master/lp_irl.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}